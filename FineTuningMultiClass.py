# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NH5OtNgMAp56XTUM8wm4BeeUDo3cOc0a
"""

!pip install evaluate

!pip install -U transformers==4.41.2 peft==0.10.0 accelerate==0.29.3 datasets evaluate

import transformers, peft, accelerate
print(transformers.__version__)
print(peft.__version__)
print(accelerate.__version__)

from datasets import load_dataset,DatasetDict,Dataset

from transformers import (
    AutoTokenizer,
    AutoConfig,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer
)

from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig
import evaluate
import torch
import numpy as np

dataset = load_dataset("lex_glue", "ecthr_a")
print(dataset)

model_checkpoint = "roberta-base"

tokenizer = AutoTokenizer.from_pretrained(
    model_checkpoint,
    add_prefix_space = True
)

if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({"pad_token":"[PAD]"})

def tokenize_function(examples):
    # 1. Join paragraphs into a single document
    texts = [" ".join(doc) for doc in examples["text"]]

    tokenizer.truncation_side = "left"
    tokenized = tokenizer(
        texts,
        truncation=True,
        max_length=512
    )

    # 2. Convert label indices to multi-hot FLOAT vectors
    num_labels = 10
    multi_hot_labels = []

    for label_list in examples["labels"]:
        label_vector = [0.0] * num_labels   # ðŸ‘ˆ FLOAT, not int
        for idx in label_list:
            label_vector[idx] = 1.0
        multi_hot_labels.append(label_vector)

    tokenized["labels"] = multi_hot_labels
    return tokenized

#Done for better outputs
id2label = {
    0: "Article_2_Right_to_Life",
    1: "Article_3_Prohibition_of_Torture",
    2: "Article_5_Right_to_Liberty",
    3: "Article_6_Right_to_Fair_Trial",
    4: "Article_8_Private_and_Family_Life",
    5: "Article_9_Freedom_of_Thought",
    6: "Article_10_Freedom_of_Expression",
    7: "Article_11_Freedom_of_Assembly",
    8: "Article_14_Non_Discrimination",
    9: "Protocol1_Article1_Property"
}

label2id = {v: k for k, v in id2label.items()}

model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint,num_labels = 10,id2label = id2label,
                                           label2id = label2id,problem_type="multi_label_classification")

model.resize_token_embeddings(len(tokenizer))

tokenized_dataset = dataset.map(tokenize_function,batched = True,remove_columns=["text"] )

from transformers import DataCollatorWithPadding

class MultiLabelDataCollator(DataCollatorWithPadding):
    def __call__(self, features):
        batch = super().__call__(features)
        batch["labels"] = batch["labels"].float()  # ðŸ‘ˆ FORCE FLOAT
        return batch

data_collator = MultiLabelDataCollator(tokenizer)

batch = data_collator([tokenized_dataset["train"][0]])
print(batch["labels"])
print(batch["labels"].dtype)

peft_config = LoraConfig(
    task_type="SEQ_CLS",
    r=8,
    lora_alpha=32,
    lora_dropout=0.05,
    target_modules=["query", "value"]
)

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

from sklearn.metrics import f1_score

def compute_metrics(eval_pred):
    logits, labels = eval_pred

    # Convert logits â†’ binary predictions (multi-label)
    preds = (logits > 0).astype(int)

    return {
        "micro_f1": f1_score(labels, preds, average="micro"),
        "macro_f1": f1_score(labels, preds, average="macro"),
    }

training_args = TrainingArguments(
    output_dir="legal-lora-multilabel",
    learning_rate=1e-4,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=5,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    logging_steps=50,
    fp16=torch.cuda.is_available()
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,   # ðŸ‘ˆ IMPORTANT
    compute_metrics=compute_metrics,
)

trainer.train()

trainer.evaluate(tokenized_dataset["test"])

